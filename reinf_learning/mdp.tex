\section{Markov-Decision Process}

\orange{MDP formally describes environment, where environment is fully observable}

I.e. current state completely characterizes that process, how the process unfolds.

Almost all RL problems can be formalized as MDPs:
\begin{itemize}
	\item Optimal control primarily deals with continuous MDPs (with continuous actions).
	\item POMDP can be converted to MDP	.
	\item \textit{Bandits} are MDPs with one state.
\end{itemize}

Idea: Markov process (chains) $\to$ Markov Reward Process (+reward function) $\to$ Markov Decision Process (+actions)

\subsection{Markov Process}

\red{Markov Property}: future is independent of the past given present.

\begin{align}
	\Prob[S_{t+1} | S_t] = 	\Prob[S_{t+1} | S_t, S_{t-1}, ..., S_{0}]
\end{align}

It means that we can define a \red{state-transition probability matrix}
\begin{align}
	\mathcal{P}_{ss'} = \Prob[S_{t+1} = s' | S_t = s]
\end{align}

\red{Markov Process (Markov Chain)} is a memory-less process defined as a tuple $(\mathcal{S}, \mathcal{P})$:
\begin{itemize}
	\item $\mathcal{S}$ is a (finite) set of states;
	\item $\mathcal{P}$ is state transition probability matrix.
\end{itemize}

This fully defines dynamics of the system.

Note: What to do if probability matrix change in time? Non-stationary Markov process (or non-stationary MDP in general).
\begin{enumerate}
	\item  You can use the same algorithms we use in the stationary case but incrementally adjust your solution to track the best solution you've found so far.
	\item or you can reduce non-stationary dynamics to a more complex Markov process\footnote{see 12:00 of the video with Facebook example, where probability of staying on Facebook reduces with time or number of visits}
\end{enumerate}

\subsection{Markov Reward Process}
	
\red{Markov Reward Process} is a Markov chain with values defined as a tuple $(\mathcal{S}, \mathcal{P}, \red{\mathcal{R}, \mathcal{\gamma}})$:
\begin{itemize}
	\item $\mathcal{R}$ is a \emph{reward function}: $\mathcal{R}_s = \E[R_{t+1} | S_t = s]$
	\begin{itemize}
		\item how much (expected\footnote{according to model??}) reward do I get from simply being in that state;
	\end{itemize}
	\item $\gamma$ is a \emph{discount factor} $\gamma \in [0, 1]$
\end{itemize}
\emph{Reward function} allows us to value judge how much reward I accumulate across a particular sequence sampled from this Markov process.

The \red{return $G_t$} is the total discounted reward from time-step $t$ (\underline{across entire chain}).
\begin{align}
	G_t = R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{align}

\begin{mynote}
	\orange{$\gamma$ helps to define $G_t$ to converge to finite!!} Myopic ($\gamma=0$ vs far-sighted ($\gamma=1$). \orange{We introduce a judgement here: we prefer short-term reward to delayed reward and amount is given by $\gamma$}
\end{mynote}