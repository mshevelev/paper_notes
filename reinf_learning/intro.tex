\section{Introduction}

\subsection{Rewards, Observations, History, State}

\orange{No supervisor, only reward signal.}
A \red{reward} $R_t$ is a scalar feedback signal: indicates how well agent is doing at step $t$. \orange{The agent’s job is to select \underline{actions} to maximize (expected) cumulative reward!!}
\begin{itemize}
	\item actions may have long term consequences, rewards may be delayed;
	\item time really matters: data sequential, not \iid like in supervised learning setting;
	\item it may be better to sacrifice \textit{immediate reward} to gain more \textit{long-term reward};
	\item agent's actions affect data it receives.
\end{itemize}

\begin{notebox}
	What if the goal is to complete a challenge in the shortest amount of time. Define reward on each step to be -1 and have a terminal state "goal completed".
\end{notebox}


The \red{history} is all the observed variables up to time $t$: $H_t = O_1 A_1, R_2 O_2 A_2, ..., R_t O_t A_t$. It is used by agent to generate next action. But it may be enormous amount of data... Instead we use \red{state} $S_t$ which is the information used to model what to do next. 

\red{State} is \underline{any} function of history: \red{$S_t = f(H_t)$}. But bad choice of state may loose relevant information contained in history.

The \red{environment state} $S^e_t$ is the environment's private internal representation, i.e. whatever it uses to generate next observation and reward. \orange{Usually it is not observed}, so agent policies cannot depend on it.

The \red{agent state} $S^a_t$ is the agent's internal representation, i.e. whatever information it uses to pick the next action. Formally, state is a function of the history \red{$S^a_t = \underbrace{f}_{\text{our choice}}(H_t)$}, where $H_t = O_1 A_1, R_2 O_2 A_2, ..., A_{t-1}, R_{t} O_{t}$, i.e. \underline{summary} of all observable variables up to time $t$. \orange{And it's our decision of what to remember, what to throw away and how to summarize the history!}

An \red{information state} (a.k.a. \textcolor{red}{Markov state}) contains all useful information from the history. $S_t$ is Markov iff
\begin{align*}
\Prob [S_{t+1}|S_t]	= \Prob[S_{t+1}| S_t, S_{t-1}, ... S_1]
\end{align*}
\begin{itemize}
	\item The future is independent of the past, given present: $H_{1:t} \to S_t \to H_{t+1:\infty}$;
	\item The state is sufficient statistics of the future (i.e. fully characterizes the future) - once the state is known the history can be thrown away;
	\item When we model about distribution of some variable in the future, conditioning on the state is the same as conditioning on the history.
\end{itemize}

\begin{notebox}
By definition...
	\begin{itemize}
		\item 	... environment state $S_t^e$ is Markov! But we don't observe it.
		\item   ... full history $H_t$ is Markov! But it is too much.
	\end{itemize}
So there is always a Markov state, it is just whether we can find a useful state.
\end{notebox}

\red{Full observability} ($\Rightarrow$ \red{MDP}) is a particular case when agent directly observes environment:
\begin{align}
	O_t = S^e_t = S^a_t
\end{align}

\red{Partial observability} ($\Rightarrow$ \red{POMDP}): agent \textit{indirectly} observes environment $S^a_t \neq S^e_t$ and has to build his own state, for example:
\begin{itemize}
	\item \red{Beliefs} of environment state: $S^a_t = \Big(\Prob[S^e_t = s^1, ..., S^e_t = s^m]\Big)$
	\item dynamic updates of most probable state estimate: $S^a_t = w_s S^a_{t-1} + w_o O_t$
\end{itemize}

\subsubsection{RL agent}
\boxedtext{
An \textcolor{red}{RL agent} may include one or more of these components:
\begin{itemize}
	\item \red{Policy}: agent’s behavior function
	\begin{itemize}
		\item map from state to action
		\item \red{deterministic}: $a = \pi(s)$
		\item \red{stochastic(probabilistic)}: $\pi(a|s) = \Prob[A_t=a|S_t=s]$
	\end{itemize}
	\item \red{Value function}: how good is each state and/or action, prediction of expected future reward
	\begin{itemize}
		\item State-value function $\pi(s) = \E[\sum_{k=0} \gamma^k R_{t+1+k} | S_t = s]$
		\item Action-value function $q(s) = \E[\sum_{k=0} \gamma^k R_{t+1+k} | S_t = s, A_t = a]$
	\end{itemize}
	\item \red{Model}: how the agent thinks the environment works
	\begin{itemize}
			\item \red{dynamics}: $\TransitionProb_{ss'}^a = \Prob[S_{t+1} = s' | S_t = s, A_t = a] $ predicts the next state
			\item \red{rewards}: $\Reward_s^a = \E_\pi[R_{t+1} | S_t = s, A_t = a] $ predicts the next immediate reward
			\item it is optional to build model of environment, there are model-free methods
	\end{itemize}
\end{itemize}
}

\begin{notebox}
	The way to interpret $\gamma$ is that we effectively look $\frac{1}{1-\gamma}$ steps into the future.
	\begin{itemize}
		\item $\gamma=0.9 \to 10 $ steps
		\item $\gamma=0.99 \to 100 $ steps
	\end{itemize}
\end{notebox}

\subsubsection{Taxonomy of RL agent}
Not all of the components above are necessary. Below is taxonomy of agents based on the presence    of certain elements.
\begin{itemize}
	\item \red{Value Based}: 
	\begin{itemize}
		\item no \underline{explicit} policy;
		\item value function;
		\item $\pi(s) = \argmax_{a \in A} \E_{s'|s,a} V(s')$.
	\end{itemize}
	\item \red{Policy Based}:
	\begin{itemize}
		\item Policy;
		\item no value function;
	\end{itemize}
	\item Actor Critic (to be discussed later)
\end{itemize}


\red{Model Free} vs \red{Model Based}. Reminder: Model = attempt to understand the environment.

\subsection{RL vs Planning}
\begin{itemize}
	\item \red{Reinforcement learning}
	\begin{itemize}
		\item The environment is initially unknown
		\item but agent improves the policy by interacting with environment
	\end{itemize}
	\item \red{Planning}
	\begin{itemize}
		\item The model of the environment is (fully) known
		\item agent interacts with this model (emulator of reality/environment) without external interaction;
		\item agent improves policy from only interaction with the model/emulator.
	\end{itemize}
\end{itemize}

These are different formulations, but of course related, as one way to solve RL problem is to learn model of environment first then do planning.

RL is like trial-and-error learning 

\subsection{Prediction vs Control}
\begin{itemize}
	\item \red{Prediction}: evaluate the future. Given fixed policy, evaluate value function.
	\item \red{Control}: optimize the future, i.e. find the best policy.
\end{itemize}

Usually we need to solve Prediction problem to solve control problem. We need to be able to \underline{evaluate} all of our policies in order to \underline{find the best one}.
