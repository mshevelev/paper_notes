\section{Introduction}

\orange{No supervisor, only reward signal.}
A \red{reward} $R_t$ is a scalar feedback signal: indicates how well agent is doing at step $t$. \orange{The agent’s job is to select \underline{actions} to maximize (expected) cumulative reward!!}
\begin{itemize}
	\item actions may have long term consequences, rewards may be delayed;
	\item time really matters: data sequential, not \iid like in supervised learning setting;
	\item it may be better to sacrifice \textit{immediate reward} to gain more \textit{long-term reward};
	\item agent's actions affect data it receives.
\end{itemize}

\begin{notebox}
	What if the goal is to complete a challenge in the shortest amount of time. Define reward on each step to be -1 and have a terminal state "goal completed".
\end{notebox}

The \red{history} is all the observed variables up to time $t$: $H_t = A_1 O_1 R_1, ..., A_t O_t R_t$. It is used by agent to generate next action. But it is not very useful since it may be enormous amount of data... Instead we use \red{state} $S_t$ which is the information used to determine what happens next. We replace the history by some concise summary that captures all the relevant information that determines what happens next.

Formally, state is \underline{any} function of history: \red{$S_t = f(H_t)$}.

The \red{environment state} $S^e_t$ is the environment's private internal representation, i.e. whatever it uses to generate next observation and reward. \orange{Usually not visible to an agent.} So it is rather a formalism that helps us understand what environment is, rather than build practical algorithms for agent. Agent policies cannot depend on the numbers describing environment state as they are unknown.

The \red{agent state} $S^a_t$ is the agent's internal representation, i.e. whatever information it uses to pick the next action. Formally, state is a function of the history \red{$S^a_t = \underbrace{f}_{\text{our choice}}(H_t)$}, where $H_t = O_1, R_1, A_1, ..., A_{t-1}, O_{t}, R_{t}$, i.e. \underline{summary} of all observable variables up to time $t$. \orange{And it's our decision of what to remember, what to throw away and how to summarize the history!}

An \red{information state} (a.k.a. \textcolor{red}{Markov state}) contains all useful information from the history. $S_t$ is Markov iff
\begin{align*}
\Prob [S_{t+1}|S_t]	= \Prob[S_{t+1}| S_t, S_{t-1}, ... S_1]
\end{align*}
\begin{itemize}
	\item The future is independent of the past, given present: $H_{1:t} \to S_t \to H_{t+1:\infty}$;
	\item once the state is known the history can be thrown away;
	\item The state is sufficient statistics of the future (i.e. fully characterizes the future);
	\item when we talk about distribution of some variable in the future, conditioning on the state is the same as conditioning on the history.
\end{itemize}


(???) POMDP (partially observable) vs MDP


\boxedtext{
An \textcolor{red}{RL agent} may include one or more of these components:
\begin{itemize}
	\item \red{Policy}: agent’s behavior function
	\begin{itemize}
		\item map from state to action
		\item \red{deterministic}: $a = \pi(s)$
		\item \red{stochastic(probabilistic)}: $\pi(a|s) = \Prob[A_t=a|S_t=s]$
	\end{itemize}
	\item \red{Value function}: how good is each state and/or action
	\begin{itemize}
		\item prediction of future reward: $v_{\pi}(s) = \EX_\pi[R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s]$
	\end{itemize}
	\item \red{Model}: agent’s representation of the environment (what the environment will do next)
	\begin{itemize}
			\item \red{dynamics}: $\mathcal{P}_{ss'}^a = \Prob[S_{t+1} = s' | S_t = s, A_t = a] $ models distribution of the next state
			\item \red{rewards}: $\mathcal{R}_s^a = \E_\pi[R_{t+1} | S_t = s, A_t = a] $ models the next immediate reward
	\end{itemize}
\end{itemize}
}

\subsection{Taxonomy of RL agent}
\begin{itemize}
	\item \red{Value Based}: 
	\begin{itemize}
		\item no \underline{explicit} policy;
		\item value function;
		\item $\pi(s) = \argmax_{a \in A} \E_{s'|s,a} V(s')$.
	\end{itemize}
	\item \red{Policy Based}:
	\begin{itemize}
		\item Policy;
		\item no value function;
	\end{itemize}
	\item Actor Critic (to be discussed later)
\end{itemize}


\red{Model Free} vs \red{Model Based}. Reminder: Model = attempt to understand the environment.

\subsection{RL vs Planning}
\begin{itemize}
	\item \red{Reinforcement learning}
	\begin{itemize}
		\item The environment is initially unknown
		\item but agent improves the policy by interacting with environment
	\end{itemize}
	\item \red{Planning}
	\begin{itemize}
		\item The model of the environment is (fully) known
		\item agent interacts with this model (emulator of reality/environment) without external interaction;
		\item agent improves policy from only interaction with the model/emulator.
	\end{itemize}
\end{itemize}

These are different formulations, but of course related, as one way to solve RL problem is to learn model of environment first then do planning.

RL is like trial-and-error learning 

\subsection{Prediction vs Control}
\begin{itemize}
	\item \red{Prediction}: evaluate the future. Given fixed policy, evaluate value function.
	\item \red{Control}: optimize the future, i.e. find the best policy.
\end{itemize}

Usually we need to solve Prediction problem to solve control problem. We need to be able to \underline{evaluate} all of our policies in order to \underline{find the best one}.
