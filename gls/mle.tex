\section{Generalized Least Squares Regression}
\subsection{Basic model}
Consider the model of data
\begin{align}
	\label{eq:basic_model}
	y &= X \beta + \epsilon  \\ 	
	\epsilon &\sim \Norm(\epsilon| 0, \Sigma) \nonumber \\
	X &\in \R^{n \times k}, y \in \R^n, \epsilon \in \R^n \nonumber 
\end{align}

In the above, $\Sigma \in \MatricesPD^n$ is a known covariance matrix of errors, $\beta \in \R^k$ is an unknown constant vector of coefficients.  Our goal is to find a good estimate of $\beta$.

Assuming $X$ is given, we have $y|X \sim \Norm(y| X\beta, \Sigma)$ and conditional log-likelihood of data $D=(X,y)$ is:


\begin{align*}
	l(\beta) = \log f(y|X; \beta) \propto -\frac{1}{2} (y - X\beta)^T \Sigma^{-1} (y - X \beta)
\end{align*}

\begin{definition}
	\label{def:gls}
	$\hat{\beta}^{GLS}$ is MLE estimate of $\beta$ in the Model \ref{eq:basic_model}: 
	\begin{align}
	\hat{\beta}^{GLS} = \argmax_\beta  l(\beta)
	\end{align} 
\end{definition}

Definition \autoref{def:gls} gives an optimization problem
\begin{align}
	\label{eq:gls_opt_problem}
	\maximize_{\beta} -\frac{1}{2} (y - X\beta)^T \Sigma^{-1} (y - X \beta)
\end{align}

the first-order condition $\triangledown_{\beta} l(\beta) = 0$ gives \textit{normalized equations}:

\begin{align*}
	X^T \Sigma^{-1} X \beta = X^T \Sigma^{-1} y 
\end{align*}

Solution to the Problem \ref{eq:gls_opt_problem} can be written as
\begin{align}
	\hat{\beta}^{GLS} = (X^T \Sigma^{-1} X)^{\dagger} X^T \Sigma^{-1} y 
\end{align}

\subsection{Comparison to OLS}
Looking at \autoref{eq:gls_opt_problem} one can note that GLS is equivalent to OLS if data $D=(X, y)$ is preprocessed. For example, using Cholesky decomposition $\Sigma = L L^T  $ we can lef-tmultiply the equation by $L^{-1}$:
\begin{align}
	y' &= X' \beta + \eps' \\
	y' &= L^{-1} y, X' = L^{-1} X \notag \\
	\eps' &= L^{-1} \eps \sim N(\eps' | 0, I) \notag
\end{align}

and the Problem \ref{eq:gls_opt_problem} reduces to the usual OLS problem: 
\begin{align}
	\label{eq:gls_opt_problem}
	\maximize_{\beta} -\frac{1}{2} (y' - X\beta)^T  (y' - X' \beta)  \Leftrightarrow \minimize_{\beta} \frac{1}{2} \norm{y' - X' \beta}_2^2
\end{align}

\subsection{Extensions}
One can extend the basic model by considering coefficient vector $\beta$ as random and introducing prior distribution\footnote{Equivalently, introduce regularization terms directly into Problem \ref{eq:gls_opt_problem}.}.

If $f(\beta) \propto const$ we get same model as above





